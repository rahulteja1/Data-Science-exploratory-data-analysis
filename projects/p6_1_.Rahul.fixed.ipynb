{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFivbbl2_nYK"
   },
   "source": [
    "# P6 \n",
    "\n",
    "\n",
    "The project explores using classification models for various tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "yyx4tHW5A0dT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(5550)\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import time\n",
    "st_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "A5O0qdQP-e1_"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   import google.colab\n",
    "#   IN_COLAB = True\n",
    "# except:\n",
    "#   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87K8Re4k-e2A",
    "outputId": "908215dc-ce7a-4e67-d0b8-29415d1db6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing otter:\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: otter-grader==4.2.0 in /usr/local/lib/python3.8/dist-packages (4.2.0)\n",
      "Requirement already satisfied: jupytext in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (1.14.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (7.1.2)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (7.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (0.4.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (0.3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (2.11.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (1.15.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (5.7.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (2.23.0)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (1.12.11)\n",
      "Requirement already satisfied: python-on-whales in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (0.55.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (1.3.5)\n",
      "Requirement already satisfied: gspread in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (3.4.2)\n",
      "Requirement already satisfied: fica>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (0.2.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from otter-grader==4.2.0) (1.14.1)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.8/dist-packages (from fica>=0.2.0->otter-grader==4.2.0) (0.17.1)\n",
      "Requirement already satisfied: sphinx in /usr/local/lib/python3.8/dist-packages (from fica>=0.2.0->otter-grader==4.2.0) (1.8.6)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->otter-grader==4.2.0) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->otter-grader==4.2.0) (2.15.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->otter-grader==4.2.0) (0.17.4)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->otter-grader==4.2.0) (2.8.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->otter-grader==4.2.0) (0.0.4)\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client->otter-grader==4.2.0) (3.19.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client->otter-grader==4.2.0) (1.57.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->otter-grader==4.2.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->otter-grader==4.2.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->otter-grader==4.2.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client->otter-grader==4.2.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->otter-grader==4.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->otter-grader==4.2.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->otter-grader==4.2.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->otter-grader==4.2.0) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->otter-grader==4.2.0) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->otter-grader==4.2.0) (3.2.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (5.6.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (2.0.10)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (0.18.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (57.4.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (4.8.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (2.6.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->otter-grader==4.2.0) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->otter-grader==4.2.0) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->otter-grader==4.2.0) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->otter-grader==4.2.0) (2.0.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from jupytext->otter-grader==4.2.0) (2.1.0)\n",
      "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.8/dist-packages (from jupytext->otter-grader==4.2.0) (0.3.3)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from jupytext->otter-grader==4.2.0) (0.10.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->jupytext->otter-grader==4.2.0) (0.1.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->otter-grader==4.2.0) (4.3.3)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->otter-grader==4.2.0) (2.16.2)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->otter-grader==4.2.0) (5.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==4.2.0) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==4.2.0) (5.10.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==4.2.0) (22.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->otter-grader==4.2.0) (3.11.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->otter-grader==4.2.0) (2.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->otter-grader==4.2.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->otter-grader==4.2.0) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas->otter-grader==4.2.0) (1.21.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->otter-grader==4.2.0) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from python-on-whales->otter-grader==4.2.0) (4.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from python-on-whales->otter-grader==4.2.0) (4.64.1)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from python-on-whales->otter-grader==4.2.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from python-on-whales->otter-grader==4.2.0) (1.10.2)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (0.7.12)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (1.4.1)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (2.11.0)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (1.2.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.2.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->sphinx->fica>=0.2.0->otter-grader==4.2.0) (3.0.9)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-websupport->sphinx->fica>=0.2.0->otter-grader==4.2.0) (1.1.5)\n"
     ]
    }
   ],
   "source": [
    "# if IN_COLAB == True: \n",
    "#     print(\"Installing otter:\")\n",
    "#     !pip install otter-grader==4.2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "i-drCvTn-e2B"
   },
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X99MnLn9rApw"
   },
   "source": [
    "# Model Evaluation (Review)\n",
    "\n",
    "*Materials copied or adpated from Applied Machine Learning in Python by Mueller*\n",
    "\n",
    "Let's review the different model evalution approaches starting from the simplistic and understand their limitations. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9EUNFX-xcO5"
   },
   "source": [
    "## 1 - Hold-out set \n",
    "\n",
    "Let's start with a simplisted approach to model evaluation, split the data into training and testing data.  \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/train_test_split_new.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "This is a common approach, but has multiple limitations.  \n",
    "\n",
    "*How to solve this problem?* Use an additional hold-out set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WYUq94UyWeD"
   },
   "source": [
    "## 2 - Three-fold split or Train/Validation/Test set \n",
    "\n",
    "Use of three separate sets:    \n",
    "* the training set for model building \n",
    "* the validation set for model selection \n",
    "* the test set for final model evaluation \n",
    "is probably the most common used method for model selection and evaluation. It is a **best practice** to follow (along with other techniques described below). \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/train_test_validation_split.png\" width=\"50%\">\n",
    "\n",
    "With this new approach, we use the validation set to select the optimum hyper-paramter and the test set to estimate the performance (accuracy).  Because the test set was not used for estimating the best hyper-parameter, the test set provides an unbiased estimate of the generalization performance. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHJb1KIS0NUY"
   },
   "source": [
    "### Example of three-fold split \n",
    "\n",
    "Let's see a simple example of using the three-fold split to select the number of neighbors in KNN on the iris data set.  We first take 25% as the test set, then take 25% of the remaining as the validation set (about ~19% of the original data).  \n",
    "\n",
    "We build a model for each value of `n_neighbors` (range from 1-15 in steps of 2), evaluate it on the validation set and store the result.  We find the value which gives the best performance. \n",
    "\n",
    "Then, we often rebuild the model on all the training data (train + validation) with the best-performing hyper-parameter (as determined by the validation set), and evaluate the model on the test set. \n",
    "\n",
    "The step of retraining the model using bot the training and validation set is optional, in particular, if model training is very expensive or if the amount of training data is large enough for our model.  In this example problem, neither is the case so we retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L5ZeC8316eE",
    "outputId": "fefe7c4f-ec15-4700-fe00-666faab5441e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation score:  1.000\n",
      "best n_neighbors: 1\n",
      "test-set score: 0.974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data \n",
    "X, y = load_iris(return_X_y = True)\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=55)\n",
    "\n",
    "# Split trainval into train + val \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=5)\n",
    "\n",
    "# create a list to hold the perf. results on validation set \n",
    "val_scores = [] \n",
    "# specify hyper-parameter values \n",
    "nbrs = np.arange(1,16,2)\n",
    "\n",
    "for n in nbrs: \n",
    "    # build a model \n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # calculate performance on validation set \n",
    "    val_scores.append(knn.score(X_val, y_val))\n",
    "\n",
    "# Find the best score and best hyper-parameter \n",
    "print(\"best validation score:  %.3f\" % np.max(val_scores))\n",
    "best_nbrs = nbrs[np.argmax(val_scores)]\n",
    "print(\"best n_neighbors: %d\" % best_nbrs)\n",
    "\n",
    "# Retrain model on train + validation set \n",
    "knn = KNeighborsClassifier(n_neighbors=best_nbrs)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(\"test-set score: %.3f\" % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh-XXLEv55Br"
   },
   "source": [
    "This approach has improved upon the hold-out set method, but still relies on the particular splits.  What is we change the random splits, we might end up with different results.  In fact, if we see different outcomes based on our splits, it may mean the model is not very robust or there is not enough data.  *How can we make it more robust?*  Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc-HIxN36fTP"
   },
   "source": [
    "## 3 - K-fold cross-validation \n",
    "\n",
    "The basic premise of cross-validation is to replace the split into training and validation data with multiple different splits.  Most commonly, cross validation is applied to the training/validation split, but it can also be applied to splitting off the test data. \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/cross_validation_new.png\" width=\"50%\">\n",
    "\n",
    "The most common variant of cross-validation is k-fold cross validation, the image above illustrates a 5-fold cross-validation.  \n",
    "\n",
    "For each fold, a split of the data is made where this fold is the validation data, and the rest is the training data.  For the 5-fold cross-validation, we split the data into five parts, and have 5 different training/validation splits.  We build a model for each of the splits using the training part and validation part to evaluate it.  The outcome is five different performance values.  These can be aggregated - compute a mean/median, or use them to estimate a variance over the splits.  \n",
    "\n",
    "This approach is more robust over using a single split.  All of the initial train/validation data is used in the validation set exactly once, where a single split only some of the data appears in the validation set.   The main disadvantage of cross-validation is the computational cost.  \n",
    "\n",
    "Another issue of k-fold cross-validation is that it doesn't produce a model, it produced k models.  If you want to make predictions on new data, how to do so?  One obvious method is to retrain on the whole train/validation set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6KiVNld2MBu"
   },
   "source": [
    "We can do cross-validation by hand, i.e., using the `KFold()` family of methods.  Alternatively, we can use the cross-validation functions: `cross_val_score` and `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2h3K6Nx2K2u",
    "outputId": "35efedbb-c763-4d8c-c13f-602cb37c1275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean validation score:  0.973\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=12) \n",
    " \n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=124)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for tr_indx, val_indx in kf.split(X_trainval, y_trainval):\n",
    "    X_train, X_val = X_trainval[tr_indx], X_trainval[val_indx]\n",
    "    y_train, y_val = y_trainval[tr_indx], y_trainval[val_indx]\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    scores.append(metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "print(\"mean validation score:  %.3f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGsHknrL_6j_"
   },
   "source": [
    "## 4 - Grid Search with Cross-validation in Validation Split\n",
    "\n",
    "Let's now think about doing model selection, but using cross-validation rather than a single split.  The overall idea is illustrated below. We still have the initial split into training and test data.  But rather than a single split into training and validation data, we run cross-validation for each parameter setting.  We record the mean score averaged over the splits in the cross-validation.  After evaluating all candidate paramters, find the one with the best mean performance.  *Keep in mind this score does not correspond to a single model; there is no best model*.  We select the hyper-parameter that is best on average over the splits.  Then we build a new model, using the hyper-parameters that performed best on average in cross-validation, on the full training dataset (X_trainval).  Finally, we evaluate this model on the test data set.   \n",
    "\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/grid_search_cross_validation_new.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/cv.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDw8rWqn_R8e",
    "outputId": "4864f3dc-c4d6-4fa2-b8c9-d8433a0eb2a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation score: 0.975\n",
      "best n_neighbors: 15\n",
      "test-set score: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=55)\n",
    "\n",
    "# create a list to hold the perf. results on validation sets \n",
    "cross_val_scores = [] \n",
    "# specify hyper-parameter values \n",
    "nbrs = np.arange(1,16,2)\n",
    "\n",
    "for n in nbrs: \n",
    "    # build the model with hyper-parameters \n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    # Instead of fitting a single model, we perform cross-validation \n",
    "    scores = cross_val_score(knn, X_trainval, y_trainval, cv=10)\n",
    "    # record the average over the 10 folds \n",
    "    cross_val_scores.append(np.mean(scores))\n",
    "\n",
    "print(f\"best cross-validation score: {np.max(cross_val_scores):.3}\")\n",
    "best_nbrs = nbrs[np.argmax(cross_val_scores)]\n",
    "print(f\"best n_neighbors: {best_nbrs}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_nbrs)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(f\"test-set score: {knn.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEgPU_G4CTZU"
   },
   "source": [
    "The code above of grid-search with cross-validation and a hold-out test set is a gold standard approach for model comparison and parameter tuning.  \n",
    "\n",
    "**ASIDE: Cross-validation vs. Grid Search** \n",
    "\n",
    "Students over conflate the use of cross-validation with the use of grid search.  These are distinct and should not be used interchangeably.  Cross-validation is a technique to robustly evaluate a particular model on a particular data set.  Grid search is a technique to tune the hyper-parameters of a particular model by brute-force search.  Often each candidate is evaluated using cross-validaiton, but it is not necessary (you could use a single split of training + validation set).  So while cross-validation is often used within a grid search, you can also do cross-validation outside of a grid search, and you can do a grid search without using cross-validation.\n",
    "\n",
    "The overall approach is illustrated below.  Start by specifying hyper-parameters to evaluate (generally this means selecting the models we are using as well).  Split the data into training and test sets.  For each hyper-parameter candidate, run a grid search on the training set, yielding a score for each split, and a mean score over all splits.  The mean validation scores are used to select the best hyper-parameter value and retrain a model on the whole training data.  Then we evaluate this final model on the test set. \n",
    "\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/gridsearch_workflow.png\" width=\"60%\">\n",
    "Image from scikit-learn.\n",
    "\n",
    "<br>\n",
    "\n",
    "This pattern of evaluation is common, therefore, `scikit-learn` has a method `GridSearchCV`, which does most of this for you. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz-Qkjb2E_ly"
   },
   "source": [
    "## 5 - GridSearchCV \n",
    "\n",
    "The `GridSearchCV` class is a meta-estimator, it takes any scikit-learn model and tunes the hyper-parameters for you using cross-validation.  The hyper-parameter grid is specified as a dictionary where the keys are the names of the parameters in the estimator and the values are all the candidate values of the hyper-parameter we want to evaluate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEdC9f5HFcqC",
    "outputId": "04e8b895-65d5-4f62-db6d-543a0fe48519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.975\n",
      "best parameters: {'n_neighbors': 15}\n",
      "test-set score: 0.967\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=55)\n",
    "\n",
    "# define the parameter grid \n",
    "param_grid = {'n_neighbors': np.arange(1, 16, 2)}\n",
    "\n",
    "# Instantiate GridSearchCV - sets up the parameters on how to run \n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, \n",
    "                    return_train_score=True)\n",
    "# Execute the search (and retrain the final model) \n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")\n",
    "\n",
    "# do a final evaluation on the test set \n",
    "print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDGwAQpBGQnp"
   },
   "source": [
    "Because grid search is a meta-estimator, after you instantiate it, you can use it like any other scikit-learn model: use `fit`, `predict`, `score` methods using the best hyper-parameter setting. \n",
    "\n",
    "The test set is reserved for the final evaluation, therefore, it can be a good idea to look at the search results without the test set.  If the `best_score_` is lower than expected or needed for an application, do not use the test set.  Also, you may want to look at whether the `best_params_` value is on the boundary of the search space specified.  If it is, you may want to extend the range.  Also, the model that was refit on the whole training + validation data (the model used when calling `predict` and `score`) is called as `best_estimator_`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO6iCn3QIkbn"
   },
   "source": [
    "## 6 - Nested Cross-Validation \n",
    "\n",
    "As mentioned above, it is most common to use cross-validation in the training/validation part of a three-fold split: train/validationa/test split.  However, it can be used for both, resulting in **nested cross-validation**.  Nested cross-validation is easy to implement, but not commonly used for three reasons:\n",
    "\n",
    "* computationally expensive, adds another loop \n",
    "* it doesn't result in a single model, so it's hard to productionize \n",
    "* it is harder to understand \n",
    "\n",
    "Here we can see an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "_AdcbraMJc0K"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors':  np.arange(1, 15, 2)}\n",
    "# instantiate grid search\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10,\n",
    "                   return_train_score=True)\n",
    "# perform cross-validation on the grid-search estimator\n",
    "# where each individual fit will internally perform cross-validation\n",
    "res = cross_validate(grid, X, y, cv=5, return_train_score=True, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OhP3qd8hJ9xL",
    "outputId": "a363a268-a61b-404a-82a6-8888b0036bfa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e34bed6d-f201-4ed0-b6fd-b8c492e5dadf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>estimator</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.816694</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.697327</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664084</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.185830</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.157663</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e34bed6d-f201-4ed0-b6fd-b8c492e5dadf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e34bed6d-f201-4ed0-b6fd-b8c492e5dadf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e34bed6d-f201-4ed0-b6fd-b8c492e5dadf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   fit_time  score_time                                          estimator  \\\n",
       "0  0.816694    0.003337  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "1  0.697327    0.004799  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "2  0.664084    0.002372  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "3  1.185830    0.003981  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "4  1.157663    0.002946  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "\n",
       "   test_score  train_score  \n",
       "0    0.966667     0.975000  \n",
       "1    1.000000     0.966667  \n",
       "2    0.933333     0.966667  \n",
       "3    0.966667     0.983333  \n",
       "4    1.000000     0.966667  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCFvTQRUJ7SH"
   },
   "source": [
    "Before, we had 8 hyper-parameter values (odds between 1-15), and 10 cross-validation folds, and a final evaluation model, so 81 models were learned.  With the outer cross-validation loop, there are 405 models, which adds time. \n",
    "\n",
    "Also, the outcome is five different scores, for each split.  However, these don't match to a single model, because the grid search may lead to different optimum parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0x5tLwm-KrL8",
    "outputId": "0c37805a-45e5-443d-e016-ef40dbf98988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_neighbors': 9},\n",
       " {'n_neighbors': 7},\n",
       " {'n_neighbors': 3},\n",
       " {'n_neighbors': 7},\n",
       " {'n_neighbors': 11}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.best_params_ for x in res['estimator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDKGTySBL_gj"
   },
   "source": [
    "In this case, there is not a model that can be immediately used on new data.  \n",
    "\n",
    "Because of these reasons we are instead going to use the **best practice is described in 4 and 5** above to select hyper-parameters and evaluate the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU8WG7xmROXI"
   },
   "source": [
    "# Data Leakage \n",
    "\n",
    "*Copied and adpated from MLmastery Data Leakage page.*\n",
    "\n",
    "A very common error when using cross-validation is data leakage. Data leakage is where information about the holdout dataset, such as a test or validation dataset, is made available to the model in the training dataset.   This is not a direct type of data leakage, where we would train the model on the test dataset. Instead, it is an indirect type of data leakage, where some knowledge about the test dataset, captured in summary statistics is available to the model during training. This can make it a harder type of data leakage to spot, especially for beginners.\n",
    "\n",
    "For example, consider a problem where we want to normalize the data, that is, scale the data to a range of 0-1.  When we normalize the input variables, this requires that we first calculate the minimum and maximum values for each variable before using these values to scale the variables. The dataset is then split into train and test datasets, but the examples in the training dataset know something about the data in the test dataset; they have been scaled by the global minimum and maximum values, so they know more about the global distribution of the variable then they should.\n",
    "\n",
    "This type of data leakage exists with almost any data preparation task, e.g., standardization or even imputation of missing values.  \n",
    "\n",
    "*How to solve this issue?*  Data preparation must be fit on the training data set only.  More generally, the entire modeling pipeline must be prepared only on the training dataset to avoid data leakage. This might include data transforms, but also other techniques such feature selection, dimensionality reduction, feature engineering and more. \n",
    "\n",
    "Let's see an example of this issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN9DxpbIToah"
   },
   "source": [
    "## Example of Data Leakage on Hold-out set\n",
    "\n",
    "We will start with some synthetic data for a binary classification problem. \n",
    "\n",
    "The naive approach for scaling the data is:    \n",
    "\n",
    "1. Run scaling on the entire data set \n",
    "2. Split the data into train/test \n",
    "3. Train the model on train, evaluate on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7mL8N8iT9or",
    "outputId": "ba056c95-fa6d-448b-8ef0-97fb5b1f0987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.800\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn import metrics\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=13, \n",
    "                           n_redundant=7, random_state=20)\n",
    "\n",
    "# normalize the dataset \n",
    "scaler = MinMaxScaler()\n",
    "Xsc = scaler.fit_transform(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xsc, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# fit the model\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLFs0qPcUexw"
   },
   "source": [
    "Let's look at how we should do the data preparation to avoid data leakage:    \n",
    "\n",
    "1. Split the data into train/test \n",
    "2. Run scaling, use train to set parameters, apply to both train and test \n",
    "3. Train the model on train, evaluate on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KciT3lAqUwPa",
    "outputId": "73358c6b-b59c-436f-c558-c817e26531bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.400\n"
     ]
    }
   ],
   "source": [
    "# GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9FT01GcV8-w"
   },
   "source": [
    "The model with data leakage has slightly better performance that that without. *Note, this may change across random splits*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KebYcHYyW4lW"
   },
   "source": [
    "## Example of Data Leakage in Cross Validation \n",
    "\n",
    "Naive data preparation with cross-validation involves applying the data transform first, then using the cross-validation procedure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oL0HQbeXLJd",
    "outputId": "3910c10b-6b94-47e2-c88c-4f6dcf5e970b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.800 (2.344)\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "Xsc = scaler.fit_transform(X)\n",
    "\n",
    "# define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, Xsc, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dSs_2mvX5SE"
   },
   "source": [
    "Let's look at the correct way to do data preparation with cross-validation. \n",
    "It requires that the data preparation method is prepared on the training set and applied to the train and test sets within the cross-validation procedure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mv-SH7TXdBYC",
    "outputId": "483150a0-b874-47c3-d509-ae30d3999497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96, 0.88, 0.99, 0.96, 0.94, 0.97, 0.94, 0.91, 0.96, 0.96, 0.93, 0.95, 0.94, 0.95, 0.93, 0.92, 0.98, 0.93, 0.98, 0.96, 0.95, 0.95, 0.94, 0.95, 0.93, 0.97, 0.98, 0.94, 0.91, 0.97]\n",
      "Accuracy: 94.767 (2.376)\n"
     ]
    }
   ],
   "source": [
    "# GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "# Set up how to perform k-fold \n",
    "kf = model_selection.RepeatedStratifiedKFold(n_splits = 10, \n",
    "                                             n_repeats=3, random_state=1)\n",
    "scores = [] \n",
    "\n",
    "# Loop over splits\n",
    "for tr_indx, te_indx in kf.split(X, y): \n",
    "    X_train, X_test = X[tr_indx], X[te_indx]\n",
    "    y_train, y_test = y[tr_indx], y[te_indx]\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_trans = scaler.transform(X_train)\n",
    "    X_test_trans = scaler.transform(X_test)\n",
    "\n",
    "    # define the model\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train_trans, y_train) \n",
    "    yhat = model.predict(X_test_trans)\n",
    "\n",
    "    scores.append(metrics.accuracy_score(y_test, yhat))\n",
    "\n",
    "print(scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqvsS6Y3izAk"
   },
   "source": [
    "## Example of Data Leakage with GridSearchCV \n",
    "\n",
    "Let's look at how we get data leakage when using GridSearchCV as discussed above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGmnIQP-jDCU",
    "outputId": "f218591f-f42f-4b76-8c9b-716ccf5de984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.9333333333333333\n",
      "best parameters: {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# Scale the data \n",
    "scaler = MinMaxScaler()\n",
    "X_trainval_sc = scaler.fit_transform(X_trainval)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the model \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# params for Grid Search \n",
    "params = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Use GridSearchCV \n",
    "grid = GridSearchCV(knn, params, cv=5, return_train_score=True)\n",
    "grid.fit(X_trainval_sc, y_trainval) \n",
    "\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4-_IepVj-Fx"
   },
   "source": [
    "What's the problem?  The scaling uses the data in train+validation to set the parameters and apply the scaling to the test set. \n",
    "\n",
    "The issue is with the GridSearchCV usage.  GridSearchCV will split the train+validation dataset into the train set and a validation set. See the image below from scikit-learn to illustrate this idea again. \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/gridsearchSV.png\" width=\"60%\">\n",
    "Image from scikit-learn. \n",
    "\n",
    "<br> \n",
    "\n",
    "Within the cross-validation, the validation set should be treated as a temporary unseen test set.  Therefore, the scaler should not be fit using this data. \n",
    "\n",
    "How do we solve data leakage in this case? \n",
    "\n",
    "Use **pipelines**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR-2gQr7vVDO"
   },
   "source": [
    "## Example of a Pipeline \n",
    "\n",
    "[Pipelines](https://scikit-learn.org/stable/data_transforms.html) allow us to use a number of different dataset transformations, we may clean, preprocess, reduce, or create feature representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdqS2QqmxMaa"
   },
   "source": [
    "### Pipeline on Hold-out set \n",
    "\n",
    "Let's see an example of scaling the data using a pipeline.  From above, we have an example of data leakage. \n",
    "\n",
    "```python\n",
    "# BAD - Example of Data Leakage\n",
    "\n",
    "# normalize the dataset \n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "# fit the model\n",
    "model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "Here, was the corrected code to prevent data leakage and compare it to a pipeline. \n",
    "\n",
    "```python \n",
    "# GOOD - Example without Data Leakage \n",
    "\n",
    "# split into train and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "# fit the model\n",
    "model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "Now below is the code implemented as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lmqhNyGwTwU",
    "outputId": "8471f56e-05c8-4bef-fc3b-1dc37b1956cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.400\n"
     ]
    }
   ],
   "source": [
    "# Pipeline to avoid data leakage\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# Setup the pipeline \n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Execute the pipeline with the data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "res = pipe.score(X_test, y_test)\n",
    "\n",
    "# evaluate predictions\n",
    "print('Accuracy: %.3f' % (res*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D73dQkLSxZ1f"
   },
   "source": [
    "### Pipeline with Cross-validation \n",
    "\n",
    "In the example below, we can see how cross-validation with data leakage can be converted to a pipeline and also how we can eliminate data leakage when using for-loops for cross-validation and in a pipeline. \n",
    "\n",
    "You will want to use the code on the right (\"GOOD\") in the future. \n",
    "\n",
    "#### Preprocessing before cross-validation *BAD - DO NOT USE*\n",
    "\n",
    "\n",
    "```python\n",
    "# BAD!\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scaler.fit_transform(X)\n",
    "\n",
    "scores = []\n",
    "for tr_indx, te_indx in KFold().split(X_sc, Y):\n",
    "    knn = KNeighborsClassifier().fit(X_sc[train], y[train])\n",
    "    score = knn.score(X_sc[test], y[test])\n",
    "    scores.append(score)\n",
    "```\n",
    "\n",
    "Which is equivalent to the following condensed code: \n",
    "\n",
    "```python\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scalar.fit_transform(X)\n",
    "scores = cross_val_score(KNeighborsClassifier(), X_sc, y)\n",
    "```\n",
    "\n",
    "#### Preprocessing within cross validation  *GOOD - USE as EXAMPLE*\n",
    "\n",
    "\n",
    "```python\n",
    "# GOOD!\n",
    "scores = []\n",
    "scaler = MinMaxScaler()\n",
    "for train, test in KFold().split(X, y):\n",
    "    scaler.fit(X[train], y[train])\n",
    "    X_sc_train = scaler.transform(X[train])\n",
    "    knn = KNeighborsClassifier().fit(X_sc_train, y[train])\n",
    "    X_sc_test = scaler.transform(X[test])\n",
    "    score = knn.score(X_sc_test, y[test])\n",
    "    scores.append(score)\n",
    "```\n",
    "\n",
    "Which is equivalent to: \n",
    "\n",
    "```python\n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "scores = cross_val_score(pipe, X, y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXQq-Icg3ml5",
    "outputId": "9b468237-a9b0-4688-b6d2-a71a1278eb53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 94.700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "scores = cross_val_score(pipe, X, y)\n",
    "\n",
    "print(\"Mean Acc: %.3f\" % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhqZzP51363U"
   },
   "source": [
    "### Pipeline with GridSearchCV \n",
    "\n",
    "If you recall, `GridSearchCV` is passed an estimator and a dictionary of parameter values for tuning hyper-parameters.  We can pass a `Pipeline` as the estimator, but we need to adjust the process above to ensure the parameter tuning is applied to the correct step of the pipeline.  This is done by specifying the hyper-parameters within a pipeline, by using the name of the step of the pipeline, followed by the double underscore ('dunder'), followed by the name of the hyper-parameter. \n",
    "\n",
    "So, when we create a pipeline and we want to tune the `n_neighbors` parameter of KNN, we need to use  `kneighborsclassifier__n_neighbors` as the hyper-parameter name. \n",
    "Below is the example code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IJNYXFr5DJY",
    "outputId": "69dcfc18-3af2-4e76-b07f-8362cc05d6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 5}\n",
      "94.39999999999999\n"
     ]
    }
   ],
   "source": [
    "# GOOD - EXAMPLE without Data Leakage using pipeline and GridSearchCV\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# create the pipeline \n",
    "knn_pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "\n",
    "# create the parameter grid \n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "params = {'kneighborsclassifier__n_neighbors': \n",
    "          [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(n_splits=10, random_state=5, shuffle=True)\n",
    "\n",
    "# Instantiate the grid-search\n",
    "grid = GridSearchCV(knn_pipe, params, cv=cvStrat)\n",
    "# run the grid search are report results \n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L5w8OTD6VQ2"
   },
   "source": [
    "We can look at all the parameters with:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35_WS_Me6aRc",
    "outputId": "b1a2ac56-bbf0-45ec-ffdd-ae269d7fa0d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('minmaxscaler', MinMaxScaler()),\n",
       "  ('kneighborsclassifier', KNeighborsClassifier())],\n",
       " 'verbose': False,\n",
       " 'minmaxscaler': MinMaxScaler(),\n",
       " 'kneighborsclassifier': KNeighborsClassifier(),\n",
       " 'minmaxscaler__clip': False,\n",
       " 'minmaxscaler__copy': True,\n",
       " 'minmaxscaler__feature_range': (0, 1),\n",
       " 'kneighborsclassifier__algorithm': 'auto',\n",
       " 'kneighborsclassifier__leaf_size': 30,\n",
       " 'kneighborsclassifier__metric': 'minkowski',\n",
       " 'kneighborsclassifier__metric_params': None,\n",
       " 'kneighborsclassifier__n_jobs': None,\n",
       " 'kneighborsclassifier__n_neighbors': 5,\n",
       " 'kneighborsclassifier__p': 2,\n",
       " 'kneighborsclassifier__weights': 'uniform'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYlRSHT26s4u"
   },
   "source": [
    "We could also tune parameters of the pre-processing step.  Here we are adding a feature selection step to choose only a percentage of the top features to be included in the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjAiBOEs6yT6",
    "outputId": "e1a3561d-28b1-49cf-90ad-5f95e6bc31f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 3, 'selectpercentile__percentile': 100}\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# create a pipeline\n",
    "select_pipe = make_pipeline(MinMaxScaler(), SelectPercentile(), \n",
    "                            KNeighborsClassifier())\n",
    "\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': [1, 3, 5, 7, 9, 10, 13, 15],\n",
    "              'selectpercentile__percentile': [1, 2, 5, 10, 50, 100]}\n",
    "\n",
    "# Instantiate grid-search, here we use default 10-fold cross-validation\n",
    "grid = GridSearchCV(select_pipe, param_grid, cv=10)\n",
    "\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZY_Wl9s7jVD"
   },
   "source": [
    "Note, we can make the parameter names of the GridSearch a bit simpler by using establishing some abbreviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRhGHWJT7sjH",
    "outputId": "5f1cd905-9575-473d-e031-da247f44724e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fs__percentile': 100, 'knn__n_neighbors': 3}\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create a pipeline\n",
    "#  Label each step of the pipeline with a name, e.g., \n",
    "#   'sc' - for scaling \n",
    "#   'fs' - for Feature Selection\n",
    "#   'knn' - for KNN classifier \n",
    "select_pipe = Pipeline([\n",
    "                        ('sc', MinMaxScaler()),\n",
    "                        ('fs', SelectPercentile()),\n",
    "                        ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'knn__n_neighbors': [1, 3, 5, 7, 9, 10, 13, 15],\n",
    "              'fs__percentile': [1, 2, 5, 10, 50, 100]}\n",
    "\n",
    "# Instantiate grid-search\n",
    "grid = GridSearchCV(select_pipe, param_grid, cv=10)\n",
    "\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzsp91JE-HWw"
   },
   "source": [
    "### Different options in a Pipeline. \n",
    "\n",
    "We may want the pipeline to select what preprocessing steps to include or what models to apply.  For example, I have been using `MinMaxScaler` in the examples, but what if instead we should use `StandardScaler` for this dataset.  We can let `GridSearchCV` answer this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uwr5m0JV-gMn",
    "outputId": "224c7b94-a4ec-468a-9690-4e672c513503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 5, 'scaler': StandardScaler()}\n",
      "95.19999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# declare a two step pipeline, explicitly giving names to both steps.\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# The name of the first step is 'scaler' and we can assign different\n",
    "# estimators to this step, such as MinMaxScaler or StandardScaler\n",
    "# There is a special value 'passthrough' which skips the step\n",
    "param_grid = {'scaler': [MinMaxScaler(), StandardScaler(), 'passthrough'],\n",
    "              # we named the second step knn, so we have to use that name here\n",
    "              'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# instantiate and run as before:\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10)\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4wrknDy--e-"
   },
   "source": [
    "Remember, we can see the detailed results with `cv_results_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ty2JCJ4U_DUp",
    "outputId": "eb62390f-a0a7-45f4-c042-a95a04e6a8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00338013, 0.00289972, 0.00236297, 0.00245512, 0.00246468,\n",
       "        0.00127387, 0.00229013, 0.00227339, 0.00123684, 0.00181355,\n",
       "        0.00261893, 0.0012305 , 0.00234799, 0.00225849, 0.00134449,\n",
       "        0.00188034, 0.00234153, 0.00126162, 0.00197246, 0.00212202,\n",
       "        0.00156476, 0.00202701, 0.00310662, 0.00107472]),\n",
       " 'std_fit_time': array([2.12713377e-03, 8.27634307e-04, 2.43916828e-03, 1.13283982e-03,\n",
       "        4.18746669e-04, 1.72442803e-04, 1.10412406e-03, 2.36465971e-04,\n",
       "        2.65264495e-04, 2.49029620e-04, 5.91676795e-04, 2.35457376e-04,\n",
       "        9.53031358e-04, 2.74919900e-04, 3.94168400e-04, 2.88842944e-04,\n",
       "        3.83327364e-04, 2.04219067e-04, 3.52003801e-04, 2.42797437e-04,\n",
       "        8.32983271e-04, 3.64427871e-04, 1.83300446e-03, 6.58665619e-05]),\n",
       " 'mean_score_time': array([0.00676978, 0.00786784, 0.00825005, 0.00737095, 0.00637209,\n",
       "        0.00719106, 0.00680301, 0.00657182, 0.00640454, 0.00613468,\n",
       "        0.0068156 , 0.0062252 , 0.00841396, 0.00642886, 0.00736365,\n",
       "        0.00653353, 0.00648527, 0.00679655, 0.00676014, 0.0064527 ,\n",
       "        0.00762146, 0.00625584, 0.00868623, 0.00624962]),\n",
       " 'std_score_time': array([0.00058636, 0.00378995, 0.00294066, 0.00122102, 0.00045836,\n",
       "        0.00196783, 0.00093368, 0.00035606, 0.00064236, 0.00047014,\n",
       "        0.00055311, 0.00033338, 0.00469123, 0.00060447, 0.00328574,\n",
       "        0.00062087, 0.00043008, 0.0004201 , 0.00048471, 0.00066865,\n",
       "        0.00229047, 0.00033094, 0.00369042, 0.00047602]),\n",
       " 'param_knn__n_neighbors': masked_array(data=[1, 1, 1, 3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9, 11, 11,\n",
       "                    11, 13, 13, 13, 15, 15, 15],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_scaler': masked_array(data=[MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'knn__n_neighbors': 1, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 1, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 1, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 3, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 3, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 3, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 5, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 5, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 5, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 7, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 7, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 7, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 9, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 9, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 9, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 11, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 11, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 11, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 13, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 13, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 13, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 15, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 15, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 15, 'scaler': 'passthrough'}],\n",
       " 'split0_test_score': array([0.94666667, 0.94666667, 0.88      , 0.94666667, 0.94666667,\n",
       "        0.89333333, 0.97333333, 0.97333333, 0.90666667, 0.98666667,\n",
       "        0.97333333, 0.92      , 0.97333333, 0.97333333, 0.88      ,\n",
       "        0.97333333, 0.97333333, 0.88      , 0.97333333, 0.97333333,\n",
       "        0.88      , 0.97333333, 0.93333333, 0.85333333]),\n",
       " 'split1_test_score': array([0.97333333, 0.97333333, 0.92      , 0.98666667, 0.98666667,\n",
       "        0.93333333, 0.96      , 0.96      , 0.93333333, 0.92      ,\n",
       "        0.94666667, 0.93333333, 0.92      , 0.93333333, 0.90666667,\n",
       "        0.93333333, 0.93333333, 0.90666667, 0.93333333, 0.94666667,\n",
       "        0.92      , 0.90666667, 0.93333333, 0.90666667]),\n",
       " 'split2_test_score': array([0.88      , 0.88      , 0.86666667, 0.90666667, 0.90666667,\n",
       "        0.92      , 0.93333333, 0.94666667, 0.92      , 0.96      ,\n",
       "        0.94666667, 0.92      , 0.97333333, 0.96      , 0.89333333,\n",
       "        0.97333333, 0.97333333, 0.92      , 0.97333333, 0.96      ,\n",
       "        0.92      , 0.96      , 0.97333333, 0.90666667]),\n",
       " 'split3_test_score': array([0.89333333, 0.90666667, 0.88      , 0.93333333, 0.94666667,\n",
       "        0.90666667, 0.96      , 0.94666667, 0.90666667, 0.93333333,\n",
       "        0.94666667, 0.92      , 0.93333333, 0.93333333, 0.92      ,\n",
       "        0.93333333, 0.93333333, 0.92      , 0.93333333, 0.93333333,\n",
       "        0.90666667, 0.93333333, 0.93333333, 0.89333333]),\n",
       " 'split4_test_score': array([0.92      , 0.92      , 0.89333333, 0.93333333, 0.92      ,\n",
       "        0.85333333, 0.90666667, 0.90666667, 0.88      , 0.92      ,\n",
       "        0.92      , 0.88      , 0.90666667, 0.90666667, 0.86666667,\n",
       "        0.90666667, 0.90666667, 0.86666667, 0.90666667, 0.89333333,\n",
       "        0.89333333, 0.92      , 0.90666667, 0.89333333]),\n",
       " 'split5_test_score': array([0.96      , 0.97333333, 0.92      , 0.97333333, 0.96      ,\n",
       "        0.93333333, 0.94666667, 0.94666667, 0.94666667, 0.96      ,\n",
       "        0.94666667, 0.93333333, 0.96      , 0.94666667, 0.92      ,\n",
       "        0.94666667, 0.96      , 0.93333333, 0.94666667, 0.94666667,\n",
       "        0.93333333, 0.93333333, 0.94666667, 0.93333333]),\n",
       " 'split6_test_score': array([0.86666667, 0.88      , 0.86666667, 0.90666667, 0.90666667,\n",
       "        0.88      , 0.90666667, 0.92      , 0.88      , 0.90666667,\n",
       "        0.90666667, 0.88      , 0.93333333, 0.93333333, 0.88      ,\n",
       "        0.90666667, 0.93333333, 0.85333333, 0.94666667, 0.93333333,\n",
       "        0.88      , 0.93333333, 0.92      , 0.86666667]),\n",
       " 'split7_test_score': array([0.90666667, 0.90666667, 0.89333333, 0.94666667, 0.89333333,\n",
       "        0.88      , 0.89333333, 0.92      , 0.89333333, 0.89333333,\n",
       "        0.89333333, 0.89333333, 0.89333333, 0.88      , 0.89333333,\n",
       "        0.88      , 0.89333333, 0.85333333, 0.88      , 0.89333333,\n",
       "        0.86666667, 0.89333333, 0.90666667, 0.86666667]),\n",
       " 'split8_test_score': array([0.90666667, 0.92      , 0.89333333, 0.97333333, 0.97333333,\n",
       "        0.94666667, 0.96      , 0.97333333, 0.96      , 0.94666667,\n",
       "        0.94666667, 0.96      , 0.93333333, 0.93333333, 0.96      ,\n",
       "        0.92      , 0.93333333, 0.97333333, 0.92      , 0.93333333,\n",
       "        0.94666667, 0.90666667, 0.90666667, 0.94666667]),\n",
       " 'split9_test_score': array([0.90666667, 0.92      , 0.94666667, 0.94666667, 0.94666667,\n",
       "        0.93333333, 0.97333333, 0.97333333, 0.96      , 1.        ,\n",
       "        0.97333333, 0.94666667, 0.98666667, 0.97333333, 0.97333333,\n",
       "        1.        , 0.98666667, 0.93333333, 1.        , 1.        ,\n",
       "        0.93333333, 1.        , 0.98666667, 0.92      ]),\n",
       " 'mean_test_score': array([0.916     , 0.92266667, 0.896     , 0.94533333, 0.93866667,\n",
       "        0.908     , 0.94133333, 0.94666667, 0.91866667, 0.94266667,\n",
       "        0.94      , 0.91866667, 0.94133333, 0.93733333, 0.90933333,\n",
       "        0.93733333, 0.94266667, 0.904     , 0.94133333, 0.94133333,\n",
       "        0.908     , 0.936     , 0.93466667, 0.89866667]),\n",
       " 'std_test_score': array([0.03268707, 0.03143954, 0.0244404 , 0.0256125 , 0.02933333,\n",
       "        0.02887521, 0.02809508, 0.02309401, 0.02887521, 0.03268707,\n",
       "        0.02476557, 0.0256125 , 0.02933333, 0.02735771, 0.03309246,\n",
       "        0.03479464, 0.02862788, 0.03761796, 0.03330666, 0.03109841,\n",
       "        0.0256125 , 0.03143954, 0.02629744, 0.02872088]),\n",
       " 'rank_test_score': array([18, 15, 24,  2, 10, 20,  5,  1, 16,  3,  9, 16,  5, 11, 19, 11,  3,\n",
       "        22,  5,  5, 20, 13, 14, 23], dtype=int32)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpTq3NBs_SK5"
   },
   "source": [
    "We can get even more advanced with our `GridSearchCV` options, because it can search over grids, and also over lists of grids (a list of dictionaries). This is useful when different pre-processing steps or models have different hyper-parameters. For example, say we wanted to tune whether the `MinMaxScaler` should scale between 0 and 1 or between -1 and 1, while also considering the case of using `StandardScaler`. We can't just add `feature_range` to the `param_grid` dictionary because `StandardScaler` doesn't have a `feature_range` parameter. Instead we can create a list of two grids: one grid that always uses `MinMaxScaler` and one that always uses `StandardScaler`. This is a bit of a contrived example, but once we know more models and transformers there will be plenty of cases where this comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Xa_MsRmq_Rue"
   },
   "outputs": [],
   "source": [
    "param_grid = [ # list of two dicts\n",
    "    # first dict always uses MinMaxScaler\n",
    "    {'scaler': [MinMaxScaler()],\n",
    "     # two options for feature_range:\n",
    "     'feature_range': [(0, 1), (-1, 1)], \n",
    "     'knn__n_neighbors': [1, 3, 5, 7, 9, 11]},\n",
    "    # second dict always uses StandardScaler\n",
    "    # there are no scaling options that we're tuning\n",
    "    {'scaler': [StandardScaler()], \n",
    "     'knn__n_neighbors': [1, 3, 5, 7, 9, 11]}   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q271UKy5AIp9"
   },
   "source": [
    "Note, the values for scaler always need to be a list, even if it's a list with a single element. So we can't specify `'scaler': MinMaxScaler()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMgo3uKhAYaI"
   },
   "source": [
    "### Accessing attributes in grid-search pipeline\n",
    "\n",
    "We may want to access information about the model. \n",
    "\n",
    "For example, we can access the model fitted on the whole training+validation data using the `best_estimator_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hciF6WBVArpP",
    "outputId": "ac2b31f9-3d6a-4106-fefa-87e0aff8fa87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15],\n",
       "                         'scaler': [MinMaxScaler(), StandardScaler(),\n",
       "                                    'passthrough']})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqGh4SdYAxP6",
    "outputId": "a6187944-68e4-4837-9638-c7d21d27db9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ItiD_rAA2Xi"
   },
   "source": [
    "You can see that best estimator is a pipeline itself.  We can also access an individual step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO_I1iMXA1I0",
    "outputId": "2f992309-4e7d-4dd3-965f-b722017cfd5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjM-swmPBKRB"
   },
   "source": [
    "This is a scaler that was fit on the whole training+validation dataset. \n",
    "\n",
    "We can also access parameters that scaler uses, for example the min values used in the `MinMaxScaler` or the mean values used in the `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEuufGvUBSPd",
    "outputId": "ec5f19e4-d5e9-4963-f3cb-7ec789c7d724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40896306,  0.36480057,  1.29951722, -0.0558038 ,  0.41304036,\n",
       "        0.58007339, -0.46107616,  0.50317085, -0.11268126, -0.09956297,\n",
       "        0.87089602, -0.71248985,  0.96428355, -1.07964908,  0.04909933,\n",
       "        1.03999126, -1.50948237, -0.05674912,  0.61672511,  0.49217227])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['scaler'].mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuBqFYRFL0x"
   },
   "source": [
    "# Exercises: Classification - Music Hits \n",
    "\n",
    "For this problem, you will work to classify a songs popularity. Specifically, you will develop methods to predict whether a song will make the Top10 of Billboards Hot 100 Chart. The data set consists of song from the Top10 of Billboards Hot 100 Chart from 1990-2010 along with a sampling of other songs that did not make the list.  \n",
    "\n",
    "The data source is the MIT 15.071 course. The data set was created by scraping Billboards Hot 100, other songs on Billboard, and using the EchoNest API, now a part of Spotify, to get song information.\n",
    "\n",
    "The variables included in the data set include several description of the song and artist (including song title and id numbers), the year the song was released. Additionally, several variables describe the song attributes: time signature, loudness, tempo, key, energy pitch, and timbre (measured of different sections of the song). The last variable is binary indicated whether the song was in the Top10 or not.\n",
    "\n",
    "You will use the variables of the song attributes to predict whether the song will be popular or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaGxJJ7BFZc9"
   },
   "source": [
    "## Q1  (5 pts) Load and understand the data \n",
    "\n",
    "Load in the `music` data. \n",
    "\n",
    "You should not use the `year`, `artistname`, `artistID`, `songtitle` or `songID` in the prediction.  \n",
    "Additionally, remove any variables that are the confidence of another variable, e.g., `timesignature_confidence`, `temp_confidence`. \n",
    "\n",
    "\n",
    "Create a input feature matrix, `Xm` and label vector `ym` that you will use to create your classifiers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "H6GJ-NZlFHmL",
    "outputId": "4c598469-4959-4662-9760-49d09eead6e9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-919d2d7e-adc2-4fcf-9291-661841b9974f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timesignature</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>key</th>\n",
       "      <th>energy</th>\n",
       "      <th>pitch</th>\n",
       "      <th>timbre_0_min</th>\n",
       "      <th>timbre_0_max</th>\n",
       "      <th>timbre_1_min</th>\n",
       "      <th>timbre_1_max</th>\n",
       "      <th>...</th>\n",
       "      <th>timbre_5_min</th>\n",
       "      <th>timbre_5_max</th>\n",
       "      <th>timbre_6_min</th>\n",
       "      <th>timbre_6_max</th>\n",
       "      <th>timbre_7_min</th>\n",
       "      <th>timbre_7_max</th>\n",
       "      <th>timbre_10_min</th>\n",
       "      <th>timbre_10_max</th>\n",
       "      <th>timbre_11_min</th>\n",
       "      <th>timbre_11_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-4.262</td>\n",
       "      <td>91.525</td>\n",
       "      <td>11</td>\n",
       "      <td>0.966656</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.002</td>\n",
       "      <td>57.342</td>\n",
       "      <td>-6.496</td>\n",
       "      <td>171.093</td>\n",
       "      <td>...</td>\n",
       "      <td>-104.683</td>\n",
       "      <td>183.089</td>\n",
       "      <td>-88.771</td>\n",
       "      <td>73.549</td>\n",
       "      <td>-71.127</td>\n",
       "      <td>82.475</td>\n",
       "      <td>-126.440</td>\n",
       "      <td>18.658</td>\n",
       "      <td>-44.770</td>\n",
       "      <td>25.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-4.051</td>\n",
       "      <td>140.048</td>\n",
       "      <td>10</td>\n",
       "      <td>0.984710</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>57.414</td>\n",
       "      <td>-37.351</td>\n",
       "      <td>171.130</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.267</td>\n",
       "      <td>42.798</td>\n",
       "      <td>-86.895</td>\n",
       "      <td>75.455</td>\n",
       "      <td>-65.807</td>\n",
       "      <td>106.918</td>\n",
       "      <td>-103.808</td>\n",
       "      <td>121.935</td>\n",
       "      <td>-38.892</td>\n",
       "      <td>22.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.571</td>\n",
       "      <td>160.512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.003</td>\n",
       "      <td>57.422</td>\n",
       "      <td>-17.222</td>\n",
       "      <td>171.060</td>\n",
       "      <td>...</td>\n",
       "      <td>-98.673</td>\n",
       "      <td>141.365</td>\n",
       "      <td>-88.874</td>\n",
       "      <td>66.504</td>\n",
       "      <td>-67.433</td>\n",
       "      <td>80.621</td>\n",
       "      <td>-108.313</td>\n",
       "      <td>33.300</td>\n",
       "      <td>-43.733</td>\n",
       "      <td>25.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.815</td>\n",
       "      <td>97.525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939207</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>57.765</td>\n",
       "      <td>-32.083</td>\n",
       "      <td>220.895</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.515</td>\n",
       "      <td>141.178</td>\n",
       "      <td>-70.790</td>\n",
       "      <td>64.540</td>\n",
       "      <td>-63.667</td>\n",
       "      <td>96.675</td>\n",
       "      <td>-102.676</td>\n",
       "      <td>46.422</td>\n",
       "      <td>-59.439</td>\n",
       "      <td>37.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-4.707</td>\n",
       "      <td>140.053</td>\n",
       "      <td>6</td>\n",
       "      <td>0.987738</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>56.872</td>\n",
       "      <td>-223.922</td>\n",
       "      <td>171.130</td>\n",
       "      <td>...</td>\n",
       "      <td>-96.147</td>\n",
       "      <td>38.303</td>\n",
       "      <td>-110.757</td>\n",
       "      <td>72.391</td>\n",
       "      <td>-55.935</td>\n",
       "      <td>110.332</td>\n",
       "      <td>-52.796</td>\n",
       "      <td>22.888</td>\n",
       "      <td>-50.414</td>\n",
       "      <td>32.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>4</td>\n",
       "      <td>-10.197</td>\n",
       "      <td>93.140</td>\n",
       "      <td>5</td>\n",
       "      <td>0.942992</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>53.462</td>\n",
       "      <td>-22.878</td>\n",
       "      <td>202.424</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.011</td>\n",
       "      <td>103.305</td>\n",
       "      <td>-130.215</td>\n",
       "      <td>59.775</td>\n",
       "      <td>-61.197</td>\n",
       "      <td>38.120</td>\n",
       "      <td>-59.707</td>\n",
       "      <td>49.414</td>\n",
       "      <td>-53.970</td>\n",
       "      <td>68.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570</th>\n",
       "      <td>4</td>\n",
       "      <td>-12.392</td>\n",
       "      <td>79.858</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>51.354</td>\n",
       "      <td>-91.916</td>\n",
       "      <td>202.639</td>\n",
       "      <td>...</td>\n",
       "      <td>-115.231</td>\n",
       "      <td>86.509</td>\n",
       "      <td>-83.905</td>\n",
       "      <td>102.373</td>\n",
       "      <td>-66.416</td>\n",
       "      <td>83.454</td>\n",
       "      <td>-97.153</td>\n",
       "      <td>36.745</td>\n",
       "      <td>-61.243</td>\n",
       "      <td>56.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>4</td>\n",
       "      <td>-10.304</td>\n",
       "      <td>91.760</td>\n",
       "      <td>2</td>\n",
       "      <td>0.736871</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>53.358</td>\n",
       "      <td>-10.087</td>\n",
       "      <td>202.877</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.989</td>\n",
       "      <td>166.003</td>\n",
       "      <td>-83.246</td>\n",
       "      <td>62.951</td>\n",
       "      <td>-69.512</td>\n",
       "      <td>103.413</td>\n",
       "      <td>-57.103</td>\n",
       "      <td>67.641</td>\n",
       "      <td>-53.729</td>\n",
       "      <td>65.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7572</th>\n",
       "      <td>4</td>\n",
       "      <td>-9.295</td>\n",
       "      <td>110.907</td>\n",
       "      <td>9</td>\n",
       "      <td>0.990053</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>52.928</td>\n",
       "      <td>-15.289</td>\n",
       "      <td>175.845</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.171</td>\n",
       "      <td>92.551</td>\n",
       "      <td>-64.419</td>\n",
       "      <td>74.428</td>\n",
       "      <td>-38.794</td>\n",
       "      <td>108.688</td>\n",
       "      <td>-83.284</td>\n",
       "      <td>56.476</td>\n",
       "      <td>-51.687</td>\n",
       "      <td>59.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7573</th>\n",
       "      <td>4</td>\n",
       "      <td>-9.762</td>\n",
       "      <td>139.650</td>\n",
       "      <td>6</td>\n",
       "      <td>0.944823</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>52.989</td>\n",
       "      <td>-32.907</td>\n",
       "      <td>195.601</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.927</td>\n",
       "      <td>202.200</td>\n",
       "      <td>-55.617</td>\n",
       "      <td>98.615</td>\n",
       "      <td>-71.984</td>\n",
       "      <td>87.098</td>\n",
       "      <td>-120.625</td>\n",
       "      <td>49.593</td>\n",
       "      <td>-47.656</td>\n",
       "      <td>70.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7574 rows  26 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-919d2d7e-adc2-4fcf-9291-661841b9974f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-919d2d7e-adc2-4fcf-9291-661841b9974f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-919d2d7e-adc2-4fcf-9291-661841b9974f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      timesignature  loudness    tempo  key    energy  pitch  timbre_0_min  \\\n",
       "0                 3    -4.262   91.525   11  0.966656  0.024         0.002   \n",
       "1                 4    -4.051  140.048   10  0.984710  0.025         0.000   \n",
       "2                 4    -3.571  160.512    2  0.989900  0.026         0.003   \n",
       "3                 4    -3.815   97.525    1  0.939207  0.013         0.000   \n",
       "4                 4    -4.707  140.053    6  0.987738  0.063         0.000   \n",
       "...             ...       ...      ...  ...       ...    ...           ...   \n",
       "7569              4   -10.197   93.140    5  0.942992  0.016         0.000   \n",
       "7570              4   -12.392   79.858    9  0.812422  0.012         0.000   \n",
       "7571              4   -10.304   91.760    2  0.736871  0.016         0.000   \n",
       "7572              4    -9.295  110.907    9  0.990053  0.061         0.000   \n",
       "7573              4    -9.762  139.650    6  0.944823  0.027         0.000   \n",
       "\n",
       "      timbre_0_max  timbre_1_min  timbre_1_max  ...  timbre_5_min  \\\n",
       "0           57.342        -6.496       171.093  ...      -104.683   \n",
       "1           57.414       -37.351       171.130  ...       -87.267   \n",
       "2           57.422       -17.222       171.060  ...       -98.673   \n",
       "3           57.765       -32.083       220.895  ...       -77.515   \n",
       "4           56.872      -223.922       171.130  ...       -96.147   \n",
       "...            ...           ...           ...  ...           ...   \n",
       "7569        53.462       -22.878       202.424  ...       -75.011   \n",
       "7570        51.354       -91.916       202.639  ...      -115.231   \n",
       "7571        53.358       -10.087       202.877  ...       -82.989   \n",
       "7572        52.928       -15.289       175.845  ...       -80.171   \n",
       "7573        52.989       -32.907       195.601  ...       -77.927   \n",
       "\n",
       "      timbre_5_max  timbre_6_min  timbre_6_max  timbre_7_min  timbre_7_max  \\\n",
       "0          183.089       -88.771        73.549       -71.127        82.475   \n",
       "1           42.798       -86.895        75.455       -65.807       106.918   \n",
       "2          141.365       -88.874        66.504       -67.433        80.621   \n",
       "3          141.178       -70.790        64.540       -63.667        96.675   \n",
       "4           38.303      -110.757        72.391       -55.935       110.332   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "7569       103.305      -130.215        59.775       -61.197        38.120   \n",
       "7570        86.509       -83.905       102.373       -66.416        83.454   \n",
       "7571       166.003       -83.246        62.951       -69.512       103.413   \n",
       "7572        92.551       -64.419        74.428       -38.794       108.688   \n",
       "7573       202.200       -55.617        98.615       -71.984        87.098   \n",
       "\n",
       "      timbre_10_min  timbre_10_max  timbre_11_min  timbre_11_max  \n",
       "0          -126.440         18.658        -44.770         25.989  \n",
       "1          -103.808        121.935        -38.892         22.513  \n",
       "2          -108.313         33.300        -43.733         25.744  \n",
       "3          -102.676         46.422        -59.439         37.082  \n",
       "4           -52.796         22.888        -50.414         32.758  \n",
       "...             ...            ...            ...            ...  \n",
       "7569        -59.707         49.414        -53.970         68.303  \n",
       "7570        -97.153         36.745        -61.243         56.902  \n",
       "7571        -57.103         67.641        -53.729         65.176  \n",
       "7572        -83.284         56.476        -51.687         59.427  \n",
       "7573       -120.625         49.593        -47.656         70.005  \n",
       "\n",
       "[7574 rows x 26 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music = pd.read_csv(\"music.csv\", encoding = \"ISO-8859-1\")\n",
    "m = music.drop(['year','artistname','artistID','songtitle','songID','timesignature_confidence','tempo_confidence','key_confidence','timbre_8_min', 'timbre_8_max', 'timbre_9_min', 'timbre_9_max',], axis=1)\n",
    "\n",
    "Xm = m.drop(['Top10'], axis=1)\n",
    "ym = m['Top10']\n",
    "\n",
    "Xm#.head()\n",
    "# Xm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "deletable": false,
    "editable": false,
    "id": "pKgujiZa-e2c",
    "outputId": "dfa07344-3380-4b7f-d113-13ed67668231"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! </p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgpeFWbNPyGk"
   },
   "source": [
    "## Q2. (40 pts) Classify Top 10 Hits \n",
    "\n",
    "We want to report out the results of predicting the top-10 hits using either KNN, Decision Trees, or SVMS.  \n",
    "\n",
    "For each model, you will tune the hyper-parameters:    \n",
    "* KNN, number of neighbors = [3, 7, 11, 15] \n",
    "* Decision Trees, maximum depth of the tree = [2, 5, 10, 25], random_state = 5\n",
    "* SVM, use a rbf kernel with C = [0.001, 0.1, 10] \n",
    "\n",
    "In addition, you will want to see which scaling methods seems to work best for this dataset and method: `StandardScaler` or `MinMaxScaler`. \n",
    "\n",
    "Overall, you will construct three pipelines to perform this analysis one for each model: KNN, DT, SVM.  You will do an initial split of your data into training+validation set with 85% of the data and a test set with 15% of the data (random_state=5).  Use 10-fold stratified cross-validation with a random_state = 5. \n",
    "\n",
    "Additionally, when selecting the best hyper-parameters, instead of using accuracy you will use the `f1_measure`.  \n",
    " \n",
    "\n",
    "One note, we are not using the results here to select a certain model (that would be using the test set for more than just estimating the generalized performance), rather just to report out the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7BKiEeL-e2c",
    "outputId": "a23ac84c-8a4b-4466-c19b-228410bb94e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 3, 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(Xm, ym, test_size=0.15, random_state = 5)\n",
    "\n",
    "# ** KNN **\n",
    "# Create pipeline\n",
    "knn_pipe = Pipeline([('scaler', MinMaxScaler()),('knn', KNeighborsClassifier())])\n",
    "\n",
    "# specify pipeline steps hyperparameters\n",
    "knn_param = {'knn__n_neighbors': [3, 7, 11, 15], 'scaler': [MinMaxScaler(), StandardScaler()]}\n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(random_state=5, n_splits=10, shuffle=True)\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "knn_grid = GridSearchCV(knn_pipe, knn_param, cv=cvStrat, scoring='f1').fit(X_trainval, y_trainval)\n",
    "\n",
    "# preditions on final test set \n",
    "knn_ytest = knn_grid.predict(X_test)\n",
    "\n",
    "print(knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEdHpeAh-e2c",
    "outputId": "fd4c28b4-4a53-4505-fffb-6a1cef0ae9f7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__max_depth': 25, 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5550)\n",
    "\n",
    "# ** DT ** \n",
    "dt_pipe = Pipeline([('scaler', MinMaxScaler()), ('dt', tree.DecisionTreeClassifier(random_state=5))])\n",
    "\n",
    "dt_param = {'dt__max_depth': [2, 5, 10, 25], 'scaler': [MinMaxScaler(), StandardScaler()]}   \n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat =  StratifiedKFold(n_splits=10, random_state=5, shuffle=True) \n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "dt_grid = GridSearchCV(dt_pipe, dt_param, cv=cvStrat, scoring='f1').fit(X_trainval, y_trainval)\n",
    "\n",
    "# preditions on final test set \n",
    "dt_ytest = dt_grid.predict(X_test)\n",
    "\n",
    "print(dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tygGEl91-e2c",
    "outputId": "018f3e08-89d5-4ae8-ad3c-2c2a12388ab2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 10, 'SVM__kernel': 'rbf', 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "# ** SVM ** \n",
    "svm_pipe = Pipeline([('scaler', MinMaxScaler()), ('SVM', svm.SVC(random_state=5))]) \n",
    "\n",
    "svm_param = {'SVM__C': [0.001, 0.1, 10], 'SVM__kernel': ['rbf'], 'scaler': [MinMaxScaler(), StandardScaler()]} \n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(n_splits=10, random_state=5, shuffle=True)\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "svm_grid = GridSearchCV(svm_pipe, svm_param, cv=cvStrat, scoring='f1').fit(X_trainval, y_trainval)\n",
    "\n",
    "# preditions on final test set\n",
    "svm_ytest = svm_grid.predict(X_test)\n",
    "\n",
    "print(svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "deletable": false,
    "editable": false,
    "id": "-cc-tOJC-e2d",
    "outputId": "acdef774-6bb5-42a6-edfe-7843bff7a2dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! </p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFX4pH4v-e2d"
   },
   "source": [
    "## Q3 - (10 pts) Table of Results \n",
    "\n",
    "Report in a DataFrame the following information for each model:\n",
    "* `Model` type (KNN, DT, SVM), \n",
    "* best `Hyper-parameters` for the model, e.g., (n_neighbors, 7), (max_depth, 10), ('C', 0.1)\n",
    "* `Accuracy`, \n",
    "* `Precision`,\n",
    "* `Recall`, \n",
    "* `F1-measure` and \n",
    "* `Balanced Acc` - balanced accuracy\n",
    "\n",
    "The last 5 values should all be calculated on the test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "bjHbZVWR-e2d",
    "outputId": "28a5f530-1f99-474e-a834-627c9c4339c6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a17406c7-c024-4c22-98b2-9f59b6b6da4c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hyper-parameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "      <th>Balanced Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors, 7</td>\n",
       "      <td>0.829376</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.302158</td>\n",
       "      <td>0.587112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>max_depth, 10</td>\n",
       "      <td>0.781003</td>\n",
       "      <td>0.316940</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.317808</td>\n",
       "      <td>0.593896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>C, 0.1</td>\n",
       "      <td>0.840809</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.247253</td>\n",
       "      <td>0.332103</td>\n",
       "      <td>0.600590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a17406c7-c024-4c22-98b2-9f59b6b6da4c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-a17406c7-c024-4c22-98b2-9f59b6b6da4c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-a17406c7-c024-4c22-98b2-9f59b6b6da4c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  Model Hyper-parameters  Accuracy  Precision    Recall  F1-measure  \\\n",
       "0   KNN   n_neighbors, 7  0.829376   0.437500  0.230769    0.302158   \n",
       "1    DT    max_depth, 10  0.781003   0.316940  0.318681    0.317808   \n",
       "2   SVM           C, 0.1  0.840809   0.505618  0.247253    0.332103   \n",
       "\n",
       "   Balanced Acc.  \n",
       "0       0.587112  \n",
       "1       0.593896  \n",
       "2       0.600590  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_test, knn_ytest)\n",
    "precision = precision_score(y_test, knn_ytest)\n",
    "recall = recall_score(y_test, knn_ytest)\n",
    "f1__score = f1_score(y_test, knn_ytest)\n",
    "Balanced_Acc = balanced_accuracy_score(y_test, knn_ytest)\n",
    "\n",
    "# Create DataFrame  \n",
    "results = pd.DataFrame({\"Model\": ['KNN', 'DT', 'SVM'],\n",
    "        \"Hyper-parameters\": ['n_neighbors, 7', 'max_depth, 10', 'C, 0.1'],\n",
    "        \"Accuracy\": [accuracy_score(y_test, knn_ytest), accuracy_score(y_test, dt_ytest), accuracy_score(y_test, svm_ytest)],\n",
    "        \"Precision\": [precision_score(y_test, knn_ytest), precision_score(y_test,  dt_ytest), precision_score(y_test, svm_ytest)],\n",
    "        \"Recall\": [recall_score(y_test, knn_ytest), recall_score(y_test, dt_ytest), recall_score(y_test, svm_ytest)],\n",
    "        \"F1-measure\": [f1_score(y_test, knn_ytest), f1_score(y_test, dt_ytest), f1_score(y_test, svm_ytest)],\n",
    "        \"Balanced Acc.\": [balanced_accuracy_score(y_test, knn_ytest), balanced_accuracy_score(y_test, dt_ytest), balanced_accuracy_score(y_test, svm_ytest)]}  ) \n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "deletable": false,
    "editable": false,
    "id": "EPZaFhth-e2d",
    "outputId": "0b4ea219-6107-4259-94db-80a1aeffe52d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! </p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS0Y3PQ3QXbR"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q4 (10 pts). Results summary \n",
    "\n",
    "Summarize the results.  Write 5-8 sentences about the results observed and the overall performance on the problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiKrsuSj-e2e"
   },
   "source": [
    "**ANSWER**\n",
    "\n",
    "In the above three moddels SVM has the highest accuraacy when compared with KNN and DT.\n",
    "\n",
    "KNN classifier using k-nearest neighbors voting. The option n neighbors = 7 was chosen since it is the least number of nearest neighbors.\n",
    "\n",
    "Support vector classification give sthe best accuracy score with rbf kernels.\n",
    "\n",
    "Decision tree has the least accuracy 0.781003 when compared with KNN and SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg3F879t-e2e"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Bonus1 (12 pts).  Improve Performance of Models\n",
    "\n",
    "The problem we are working with deals with an imbalanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXdRLCvB-e2e",
    "outputId": "38300dcf-8256-4ce5-a84f-8c397da6bff5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.774227620808029"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npos = len(ym[ym==1])\n",
    "nneg = len(ym[ym==0])\n",
    "# Percentage of positive samples in data set\n",
    "npos / (nneg + npos)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr96gUW--e2e"
   },
   "source": [
    "The imbalanced data is one explanation for the poor performance of our classifiers above (among other reasons).  \n",
    "\n",
    "Let's try to improve this performance.  Classification with imbalanced data can be improved using a number of different techniques.  Two approaches are: \n",
    "\n",
    "* Cost-sensitive or weighted learning approach\n",
    "* Data or sampling approach \n",
    "\n",
    "Here we will examing the class weighting approach. Some of our traditional classification models are adapted to include a penalty of cost for the different classes.  In our problem, we have a minority class \"Top10 Hits\" and the majority class \"Non-Top10 Hits\".  We can use class weighting to penalize the model for misclassifying the minority class more than the majority class.  \n",
    "\n",
    "We will make use of the `scikit-learn` parameter `class_weight`.  Setting `class_weight ='balanced'` will have a weight applied inversely proportional to the class frequency.  \n",
    "\n",
    "Note, not all classification models have this parameter to set, e.g., KNN. \n",
    "\n",
    "Rerun your DT and SVM pipelines from above (Q2) now with the DT and SVM using the parameter `class_weight ='balanced'`\n",
    "\n",
    "Add the resulting models `DT bal class weights` and `SVM bal class weights` and their performance to the results table from Q3.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_i02mNf-e2f",
    "outputId": "b42edb9b-592e-45d2-beec-8b1c18e45875",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__class_weight': 'balanced', 'dt__max_depth': 10, 'scaler': MinMaxScaler()}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# ** DT ** \n",
    "# include class_weight in DT parameters\n",
    "dt_pipe2 = Pipeline([('scaler', MinMaxScaler()), ('dt', tree.DecisionTreeClassifier(random_state=5))]) \n",
    "\n",
    "dt_param2 = {'dt__max_depth': [2, 5, 10, 25], 'scaler': [MinMaxScaler(), StandardScaler()], 'dt__class_weight': ['balanced', None]}   \n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(n_splits=10, random_state=5, shuffle=True)\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "dt_grid2 = GridSearchCV(dt_pipe2, dt_param2, cv=cvStrat, scoring='f1').fit(X_trainval, y_trainval)\n",
    "\n",
    "# preditions on final test set \n",
    "dt_ytest2 = dt_grid2.predict(X_test)\n",
    "\n",
    "\n",
    "print(dt_grid2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyrEJmNC8L3C",
    "outputId": "cba6234c-cd46-4c17-b85d-63113448f961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scaler': MinMaxScaler(), 'svm__C': 10, 'svm__class_weight': 'balanced', 'svm__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ** SVM ** \n",
    "# include class_weight in SVM parameters\n",
    "svm_pipe2 = Pipeline([ ('scaler', MinMaxScaler()), ('svm', svm.SVC(random_state=5))]) \n",
    "\n",
    "svm_param2 = {'svm__C': [0.001, 0.1, 10],'scaler': [MinMaxScaler(), StandardScaler()],'svm__kernel': ['rbf'],'svm__class_weight': ['balanced']} \n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(n_splits=10, random_state=5, shuffle=True)\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "svm_grid2 = GridSearchCV(svm_pipe2, svm_param2, cv=cvStrat, scoring='f1').fit(X_trainval, y_trainval)\n",
    "\n",
    "# preditions on final test set\n",
    "svm_ytest2 = svm_grid2.predict(X_test)\n",
    "\n",
    "print(svm_grid2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFFP7hwM-e2f",
    "outputId": "5dea9f66-15cd-4e79-b89a-a2a6638326b4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9478d759-5ee2-4f59-8fdd-bc7eccdfbe95\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hyper-parameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "      <th>Balanced Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors, 7</td>\n",
       "      <td>0.829376</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.302158</td>\n",
       "      <td>0.587112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>max_depth, 10</td>\n",
       "      <td>0.781003</td>\n",
       "      <td>0.316940</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.317808</td>\n",
       "      <td>0.593896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>C, 0.1</td>\n",
       "      <td>0.840809</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.247253</td>\n",
       "      <td>0.332103</td>\n",
       "      <td>0.600590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT bal class weights</td>\n",
       "      <td>max_depth, 10</td>\n",
       "      <td>0.687775</td>\n",
       "      <td>0.275325</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.373898</td>\n",
       "      <td>0.645135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM bal class weights</td>\n",
       "      <td>C, 10</td>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.457926</td>\n",
       "      <td>0.710434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9478d759-5ee2-4f59-8fdd-bc7eccdfbe95')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9478d759-5ee2-4f59-8fdd-bc7eccdfbe95 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9478d759-5ee2-4f59-8fdd-bc7eccdfbe95');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                   Model Hyper-parameters  Accuracy  Precision    Recall  \\\n",
       "0                    KNN   n_neighbors, 7  0.829376   0.437500  0.230769   \n",
       "1                     DT    max_depth, 10  0.781003   0.316940  0.318681   \n",
       "2                    SVM           C, 0.1  0.840809   0.505618  0.247253   \n",
       "3   DT bal class weights    max_depth, 10  0.687775   0.275325  0.582418   \n",
       "4  SVM bal class weights            C, 10  0.756376   0.355623  0.642857   \n",
       "\n",
       "   F1-measure  Balanced Acc.  \n",
       "0    0.302158       0.587112  \n",
       "1    0.317808       0.593896  \n",
       "2    0.332103       0.600590  \n",
       "3    0.373898       0.645135  \n",
       "4    0.457926       0.710434  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, knn_ytest)\n",
    "precision = precision_score(y_test, knn_ytest)\n",
    "recall = recall_score(y_test, knn_ytest)\n",
    "f1__score = f1_score(y_test, knn_ytest)\n",
    "Balanced_Acc = balanced_accuracy_score(y_test, knn_ytest)\n",
    "\n",
    "# Create DataFrame  \n",
    "results = pd.DataFrame({'Model': ['KNN', 'DT', 'SVM', 'DT bal class weights','SVM bal class weights'],\n",
    "        'Hyper-parameters': ['n_neighbors, 7', 'max_depth, 10', 'C, 0.1','max_depth, 10', 'C, 10'],\n",
    "        'Accuracy': [accuracy_score(y_test, knn_ytest), accuracy_score(y_test, dt_ytest), accuracy_score(y_test, svm_ytest), accuracy_score(y_test, dt_ytest2), accuracy_score(y_test, svm_ytest2)],\n",
    "        'Precision': [precision_score(y_test, knn_ytest), precision_score(y_test,  dt_ytest), precision_score(y_test, svm_ytest), precision_score(y_test,  dt_ytest2), precision_score(y_test, svm_ytest2)],\n",
    "        'Recall': [recall_score(y_test, knn_ytest), recall_score(y_test, dt_ytest), recall_score(y_test, svm_ytest), recall_score(y_test, dt_ytest2), recall_score(y_test, svm_ytest2)],\n",
    "        'F1-measure': [f1_score(y_test, knn_ytest), f1_score(y_test, dt_ytest), f1_score(y_test, svm_ytest), f1_score(y_test, dt_ytest2), f1_score(y_test, svm_ytest2)],\n",
    "        'Balanced Acc.': [balanced_accuracy_score(y_test, knn_ytest), balanced_accuracy_score(y_test, dt_ytest), balanced_accuracy_score(y_test, svm_ytest), balanced_accuracy_score(y_test, dt_ytest2), balanced_accuracy_score(y_test, svm_ytest2)]}  ) \n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "deletable": false,
    "editable": false,
    "id": "PbHomYUm-e2f",
    "outputId": "8aa08b7d-123b-4762-d011-1b500b30a0eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>b1</pre></strong> passed! </p>"
      ],
      "text/plain": [
       "b1 results: All test cases passed!"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoP6ce4m-e2f"
   },
   "source": [
    "## Bonus2 (18 pts). Improve Model Performance (part 2)\n",
    "\n",
    "For this problem, you can improve the model performance by using a data or sampling approach. \n",
    "\n",
    "We will make use of the package `imbalanced-learn` to help with the sampling methods. \n",
    "[`imbalanced-learn`](https://imbalanced-learn.org/stable/) extends `scikit-learn` to provide tools for classification with imbalanced data. \n",
    "\n",
    "In general, the data or sampling approach tries to correct the imbalance in the data with two main methods: \n",
    "\n",
    "* Oversampling - generating new synthetic examples from the minority class \n",
    "* Undersampling - reduce the number of samples in the majority class to match the minority class \n",
    "\n",
    "Both methods have their advantages and disadvantages. \n",
    "\n",
    "We will make use of a commen techniques known as SMOTE - Synthetic Minority Oversampling TEchnique.  \n",
    "https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "\n",
    "Refactor your code for Q2 to now add a sampling step to your pipeline.  This step will use SMOTE with the default options (random_state=5).  \n",
    "\n",
    "You will now use the `ImbPipeline()` function (renaming of the `Pipeline` function in `imbalanced-learn`).  \n",
    "\n",
    "Make pipelines for KNN, DT, and SVM with the same hyper-parameters as above.  For this problem, you will not use the `class_weight` option.  \n",
    "\n",
    "Add your results as new rows to the results table for `KNN SMOTE`, `DT SMOTE`, and `SVM SMOTE`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6HcfDj4-e2f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ytakRDW-e2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ** KNN ** \n",
    "# Include SMOTE as a step after scaling in the pipeline\n",
    "# Create pipeline\n",
    "knn_pipe3 = ImbPipeline(...)\n",
    "\n",
    "# specify pipeline steps hyperparameters\n",
    "knn_param3 = ...\n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = ...\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "knn_grid3 = ...\n",
    "\n",
    "\n",
    "# preditions on final test set \n",
    "knn_ytest3 = ...\n",
    "\n",
    "\n",
    "print(knn_grid3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhcpJXOQ-e2g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ** DT ** \n",
    "# Include SMOTE as a step after scaling in the pipeline\n",
    "#   set random_state=5 for SMOTE\n",
    "dt_pipe3 = ImbPipeline(...)\n",
    "\n",
    "dt_param3 = ...  \n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = ...\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "dt_grid3 =...\n",
    "\n",
    "# preditions on final test set \n",
    "dt_ytest3 = ...\n",
    "\n",
    "\n",
    "print(dt_grid3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP1jV2Cs-e2g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ** SVM ** \n",
    "# include SMOTE as a step after scaling in the pipeline\n",
    "#   set random_state=5 for SMOTE\n",
    "svm_pipe3 = ImbPipeline(...)\n",
    "\n",
    "svm_param3 = ...\n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = ...\n",
    "\n",
    "# instantiate and run GridSearchCV on pipeline:\n",
    "svm_grid3 = ...\n",
    "\n",
    "# preditions on final test set\n",
    "svm_ytest3 = ...\n",
    "\n",
    "\n",
    "print(svm_grid3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PuW5n30-e2g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add \"DT SMOTE\" and \"SVM SMOTE\" rows \n",
    "#  to the results table. \n",
    "\n",
    "results = ...\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ppnuJ2DD-e2g"
   },
   "outputs": [],
   "source": [
    "grader.check(\"b2\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "un5550f22",
   "language": "python",
   "name": "un5550f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "b1": {
     "name": "b1",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> results.shape == (5,7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dt_grid2.best_params_['dt__max_depth'] == 10\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> svm_grid2.best_params_['svm__C'] == 10\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "b2": {
     "name": "b2",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> results.shape == (8,7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(results.columns == ['Model', 'Hyper-parameters', 'Accuracy', 'Precision', 'Recall',\n...        'F1-measure', 'Balanced Acc.'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dt_grid3.best_params_['dt__max_depth'] == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> svm_grid3.best_params_['svm__C'] == 0.1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> knn_grid3.best_params_['knn__n_neighbors'] == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1": {
     "name": "q1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xm.shape == (7574, 26)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ym.shape[0] == 7574\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(Xm.columns == ['timesignature', 'loudness', 'tempo', 'key', 'energy', 'pitch',\n...        'timbre_0_min', 'timbre_0_max', 'timbre_1_min', 'timbre_1_max',\n...        'timbre_2_min', 'timbre_2_max', 'timbre_3_min', 'timbre_3_max',\n...        'timbre_4_min', 'timbre_4_max', 'timbre_5_min', 'timbre_5_max',\n...        'timbre_6_min', 'timbre_6_max', 'timbre_7_min', 'timbre_7_max',\n...        'timbre_10_min', 'timbre_10_max', 'timbre_11_min', 'timbre_11_max'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(ym[109:112] == [0,0,0])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 20,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> knn_grid.best_params_['knn__n_neighbors'] == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> temp = str(knn_grid.best_params_['scaler'])\n>>> temp.startswith(\"Stand\")\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(metrics.roc_auc_score(y_test, knn_ytest), 0.58711236)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> results.shape == (3,7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(results.columns == ['Model', 'Hyper-parameters', 'Accuracy', 'Precision', \n...                     'Recall', 'F1-measure', 'Balanced Acc.'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(results.iloc[0,6], 0.587112)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
